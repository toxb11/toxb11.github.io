[{"content":"What is Map Reduce? In summaryl, Map Reduce(MR) will process input files to a set of key-value pairs.\n","permalink":"/posts/tech/introducing-mapreduce-a-parallel-distributed-algorithm/","summary":"What is Map Reduce? In summaryl, Map Reduce(MR) will process input files to a set of key-value pairs.","title":"Introducing MapReduce   a Parallel, Distributed Algorithm"},{"content":" 这篇文章记录Pod的删除流程。kubernetes版志1.23.4\n记录删除时间DeletionTimestamp  执行删除时，kubelet会向apiSercer发送一次delete请求。ApiServer的处理逻辑位于staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92  func (e *Store) Delete(ctx context.Context, name string, deleteValidation rest.ValidateObjectFunc, options *metav1.DeleteOptions) (runtime.Object, bool, error) {  key, err := e.KeyFunc(ctx, name)  if err != nil {  return nil, false, err  }  obj := e.NewFunc()  qualifiedResource := e.qualifiedResourceFromContext(ctx)  if err = e.Storage.Get(ctx, key, storage.GetOptions{}, obj); err != nil {  return nil, false, storeerr.InterpretDeleteError(err, qualifiedResource, name)  }  // 如果待资源的资源已经为nil，则立即删除  if options == nil {  options = metav1.NewDeleteOptions(0)  }  var preconditions storage.Preconditions  if options.Preconditions != nil {  preconditions.UID = options.Preconditions.UID  preconditions.ResourceVersion = options.Preconditions.ResourceVersion  }  // 判断是否需要优雅删除，判断的标准是 DeletionGracePeriodSeconds 值是否为 0  // 对于pod而言，默认的优雅时间是30s，因此这里还不会立即删除掉  // 而是将DeletionTimestamp 设置为当前时间  graceful, pendingGraceful, err := rest.BeforeDelete(e.DeleteStrategy, ctx, obj, options)  if err != nil {  return nil, false, err  }  // this means finalizers cannot be updated via DeleteOptions if a deletion is already pending  if pendingGraceful {  out, err := e.finalizeDelete(ctx, obj, false, options)  return out, false, err  }  // check if obj has pending finalizers  accessor, err := meta.Accessor(obj)  if err != nil {  return nil, false, apierrors.NewInternalError(err)  }  pendingFinalizers := len(accessor.GetFinalizers()) != 0  var ignoreNotFound bool  var deleteImmediately bool = true  var lastExisting, out runtime.Object   // Handle combinations of graceful deletion and finalization by issuing  // the correct updates.  shouldUpdateFinalizers, _ := deletionFinalizersForGarbageCollection(ctx, e, accessor, options)  // TODO: remove the check, because we support no-op updates now.  if graceful || pendingFinalizers || shouldUpdateFinalizers {  err, ignoreNotFound, deleteImmediately, out, lastExisting = e.updateForGracefulDeletionAndFinalizers(ctx, name, key, options, preconditions, deleteValidation, obj)  // Update the preconditions.ResourceVersion if set since we updated the object.  if err == nil \u0026amp;\u0026amp; deleteImmediately \u0026amp;\u0026amp; preconditions.ResourceVersion != nil {  accessor, err = meta.Accessor(out)  if err != nil {  return out, false, apierrors.NewInternalError(err)  }  resourceVersion := accessor.GetResourceVersion()  preconditions.ResourceVersion = \u0026amp;resourceVersion  }  }   // !deleteImmediately covers all cases where err != nil. We keep both to be future-proof.  if !deleteImmediately || err != nil {  return out, false, err  }   // Going further in this function is not useful when we are  // performing a dry-run request. Worse, it will actually  // override \u0026#34;out\u0026#34; with the version of the object in database  // that doesn\u0026#39;t have the finalizer and deletiontimestamp set  // (because the update above was dry-run too). If we already  // have that version available, let\u0026#39;s just return it now,  // otherwise, we can call dry-run delete that will get us the  // latest version of the object.  if dryrun.IsDryRun(options.DryRun) \u0026amp;\u0026amp; out != nil {  return out, true, nil  }   // delete immediately, or no graceful deletion supported  klog.V(6).InfoS(\u0026#34;Going to delete object from registry\u0026#34;, \u0026#34;object\u0026#34;, klog.KRef(genericapirequest.NamespaceValue(ctx), name))  out = e.NewFunc()  if err := e.Storage.Delete(ctx, key, out, \u0026amp;preconditions, storage.ValidateObjectFunc(deleteValidation), dryrun.IsDryRun(options.DryRun), nil); err != nil {  // Please refer to the place where we set ignoreNotFound for the reason  // why we ignore the NotFound error .  if storage.IsNotFound(err) \u0026amp;\u0026amp; ignoreNotFound \u0026amp;\u0026amp; lastExisting != nil {  // The lastExisting object may not be the last state of the object  // before its deletion, but it\u0026#39;s the best approximation.  out, err := e.finalizeDelete(ctx, lastExisting, true, options)  return out, true, err  }  return nil, false, storeerr.InterpretDeleteError(err, qualifiedResource, name)  }  out, err = e.finalizeDelete(ctx, out, true, options)  return out, true, err }   优雅删除  由于 Pod 中涉及到其他很多资源，比如 sandbox 容器、volume 卷等等，在删除后都需要进行回收，而删除 Pod 最终也是去删除对应的容器，这个就需要 Pod 所在节点的 kubelet 来完成清理了。kubelet 首先同样会一直 watch 我们的 Pod，当 Pod 的删除时间更新后，自然就会接收到事件，然后进行相应的清理工作。\n kubelet 对 Pod 的处理主要在 syncLoop 函数中，会去调用和事件相关的处理函数 syncLoopIteration\n 当执行删除操作的时候，apiserver 首先会更新 Pod 中的 DeletionTimestamp 属性，这个改变对于 kubelet 来说属于更新操作，所以会对应 kubetypes.UPDATE 操作，会调用 HandlePodUpdates 函数进行更新。\n 在 HandlePodUpdates 中会调用 dispatchWork 将 Pod 删除分配给具体的 worker 处理，podWorker 是具体的执行者，也就是每次 Pod 需要更新都会发送给 podWorker。\n podWorker在接收到删除操作的UpdateOption之后，会进行killPod的方法调用。\n1 2 3 4 5 6 7 8 9 10  func (kl *Kubelet) killPod(pod *v1.Pod, p kubecontainer.Pod, gracePeriodOverride *int64) error {  // 调用容器运行时去停掉Pod中的容器  if err := kl.containerRuntime.KillPod(pod, p, gracePeriodOverride); err != nil {  return err  }  if err := kl.containerManager.UpdateQOSCgroups(); err != nil {  klog.V(2).InfoS(\u0026#34;Failed to update QoS cgroups while killing pod\u0026#34;, \u0026#34;err\u0026#34;, err)  }  return nil }    在kl.containerRuntime.KillPod 中，先杀掉所用运行的容器，然后删除Pod的SandBox，kill容器时，会为每个容器开启一个goroutine来进行删除，删除中会先执行pre-stop的hook，最后再停止，这一步才调用CRI来停止容器（pkg/kubelet/kuberuntime/kuberuntime_container.go line:717）。\n 容器停掉之后，接下来就会去调用运行时服务的 StopPodSandbox 函数停止 sandbox 容器。\n1 2 3 4 5 6 7  // Stop all sandboxes belongs to same pod for _, podSandbox := range runningPod.Sandboxes {  if err := m.runtimeService.StopPodSandbox(podSandbox.ID.ID); err != nil \u0026amp;\u0026amp; !crierror.IsNotFound(err) {  killSandboxResult.Fail(kubecontainer.ErrKillPodSandbox, err.Error())  klog.ErrorS(nil, \u0026#34;Failed to stop sandbox\u0026#34;, \u0026#34;podSandboxID\u0026#34;, podSandbox.ID)  } }    到这里 kubelet 就完成了对 Pod 的优雅删除，但是这并没有结束。\n同步删除结果  对于优雅删除一开始在 apiserver 只是给 Pod 设置了 DeletionTimestamp 属性，然后 kubelet watch 来更新后去完成了 Pod 的优雅删除，但是现在服务端中还有 Pod 的记录，并没有真正去删除。\n 在 kubelet 启动的时候同时还去启动了一个 statusManager 的同步循环，该 Manager 是 kubelet pod 状态的真实来源，应该与最新的 v1.PodStatus 保持同步，它还将更新同步回 apiserver，也就是当优雅删除完成后我们还将通过该管理器将状态同步回 apiserver。\n 状态管理器在与 apiserver 进行状态同步的时候会去调用该管理器下面的 syncPod 方法进行处理，代码位于 pkg/kubelet/status/status_manager.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  func (m *manager) Start() {  // Don\u0026#39;t start the status manager if we don\u0026#39;t have a client. This will happen  // on the master, where the kubelet is responsible for bootstrapping the pods  // of the master components.  if m.kubeClient == nil {  klog.InfoS(\u0026#34;Kubernetes client is nil, not starting status manager\u0026#34;)  return  }   klog.InfoS(\u0026#34;Starting to sync pod status with apiserver\u0026#34;)   //nolint:staticcheck // SA1015 Ticker can leak since this is only called once and doesn\u0026#39;t handle termination.  syncTicker := time.NewTicker(syncPeriod).C   // syncPod and syncBatch share the same go routine to avoid sync races.  go wait.Forever(func() {  for {  select {  case syncRequest := \u0026lt;-m.podStatusChannel:  klog.V(5).InfoS(\u0026#34;Status Manager: syncing pod with status from podStatusChannel\u0026#34;,  \u0026#34;podUID\u0026#34;, syncRequest.podUID,  \u0026#34;statusVersion\u0026#34;, syncRequest.status.version,  \u0026#34;status\u0026#34;, syncRequest.status.status)  ===============  m.syncPod(syncRequest.podUID, syncRequest.status)  ===============  case \u0026lt;-syncTicker:  klog.V(5).InfoS(\u0026#34;Status Manager: syncing batch\u0026#34;)  // remove any entries in the status channel since the batch will handle them  for i := len(m.podStatusChannel); i \u0026gt; 0; i-- {  \u0026lt;-m.podStatusChannel  }  m.syncBatch()  }  }  }, 0) }   在syncPod中会依次检查\n  是否有容器运行\n  volumes是否清理\n  pod cgroup 是否清理\n   检查成功将向 apiserver 发送 Delete 请求，再次删除 Pod 了。\n 不过这一次的设置的 GracePeriodSeconds 为 0，表示要强制删除 Pod 了，到这里 apiserver 会再次收到 DELETE 请求，与第一次不同的是，这次是强制删除 Pod，会去 etcd 中删除 Pod 对象了。\n 同时还会调用 probeManager 去移除 Pod 相关的探针 prober worker，到这一步接受就表示 Pod 彻底从节点上删除了。\n","permalink":"/posts/tech/pod%E5%88%A0%E9%99%A4%E6%B5%81%E7%A8%8B/","summary":"这篇文章记录Pod的删除流程。kubernetes版志1.23.4 记录删除时间DeletionTimestamp 执行删除时，kubelet会","title":"Pod删除流程"},{"content":"这篇文章记录Pod的删除流程。kubernetes版志1.23.4\n记录删除时间DeletionTimestamp  执行删除时，kubelet会向apiSercer发送一次delete请求。ApiServer的处理逻辑位于staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92  func (e *Store) Delete(ctx context.Context, name string, deleteValidation rest.ValidateObjectFunc, options *metav1.DeleteOptions) (runtime.Object, bool, error) {  key, err := e.KeyFunc(ctx, name)  if err != nil {  return nil, false, err  }  obj := e.NewFunc()  qualifiedResource := e.qualifiedResourceFromContext(ctx)  if err = e.Storage.Get(ctx, key, storage.GetOptions{}, obj); err != nil {  return nil, false, storeerr.InterpretDeleteError(err, qualifiedResource, name)  }  // 如果待资源的资源已经为nil，则立即删除  if options == nil {  options = metav1.NewDeleteOptions(0)  }  var preconditions storage.Preconditions  if options.Preconditions != nil {  preconditions.UID = options.Preconditions.UID  preconditions.ResourceVersion = options.Preconditions.ResourceVersion  }  // 判断是否需要优雅删除，判断的标准是 DeletionGracePeriodSeconds 值是否为 0  // 对于pod而言，默认的优雅时间是30s，因此这里还不会立即删除掉  // 而是将DeletionTimestamp 设置为当前时间  graceful, pendingGraceful, err := rest.BeforeDelete(e.DeleteStrategy, ctx, obj, options)  if err != nil {  return nil, false, err  }  // this means finalizers cannot be updated via DeleteOptions if a deletion is already pending  if pendingGraceful {  out, err := e.finalizeDelete(ctx, obj, false, options)  return out, false, err  }  // check if obj has pending finalizers  accessor, err := meta.Accessor(obj)  if err != nil {  return nil, false, apierrors.NewInternalError(err)  }  pendingFinalizers := len(accessor.GetFinalizers()) != 0  var ignoreNotFound bool  var deleteImmediately bool = true  var lastExisting, out runtime.Object   // Handle combinations of graceful deletion and finalization by issuing  // the correct updates.  shouldUpdateFinalizers, _ := deletionFinalizersForGarbageCollection(ctx, e, accessor, options)  // TODO: remove the check, because we support no-op updates now.  if graceful || pendingFinalizers || shouldUpdateFinalizers {  err, ignoreNotFound, deleteImmediately, out, lastExisting = e.updateForGracefulDeletionAndFinalizers(ctx, name, key, options, preconditions, deleteValidation, obj)  // Update the preconditions.ResourceVersion if set since we updated the object.  if err == nil \u0026amp;\u0026amp; deleteImmediately \u0026amp;\u0026amp; preconditions.ResourceVersion != nil {  accessor, err = meta.Accessor(out)  if err != nil {  return out, false, apierrors.NewInternalError(err)  }  resourceVersion := accessor.GetResourceVersion()  preconditions.ResourceVersion = \u0026amp;resourceVersion  }  }   // !deleteImmediately covers all cases where err != nil. We keep both to be future-proof.  if !deleteImmediately || err != nil {  return out, false, err  }   // Going further in this function is not useful when we are  // performing a dry-run request. Worse, it will actually  // override \u0026#34;out\u0026#34; with the version of the object in database  // that doesn\u0026#39;t have the finalizer and deletiontimestamp set  // (because the update above was dry-run too). If we already  // have that version available, let\u0026#39;s just return it now,  // otherwise, we can call dry-run delete that will get us the  // latest version of the object.  if dryrun.IsDryRun(options.DryRun) \u0026amp;\u0026amp; out != nil {  return out, true, nil  }   // delete immediately, or no graceful deletion supported  klog.V(6).InfoS(\u0026#34;Going to delete object from registry\u0026#34;, \u0026#34;object\u0026#34;, klog.KRef(genericapirequest.NamespaceValue(ctx), name))  out = e.NewFunc()  if err := e.Storage.Delete(ctx, key, out, \u0026amp;preconditions, storage.ValidateObjectFunc(deleteValidation), dryrun.IsDryRun(options.DryRun), nil); err != nil {  // Please refer to the place where we set ignoreNotFound for the reason  // why we ignore the NotFound error .  if storage.IsNotFound(err) \u0026amp;\u0026amp; ignoreNotFound \u0026amp;\u0026amp; lastExisting != nil {  // The lastExisting object may not be the last state of the object  // before its deletion, but it\u0026#39;s the best approximation.  out, err := e.finalizeDelete(ctx, lastExisting, true, options)  return out, true, err  }  return nil, false, storeerr.InterpretDeleteError(err, qualifiedResource, name)  }  out, err = e.finalizeDelete(ctx, out, true, options)  return out, true, err }   优雅删除  由于 Pod 中涉及到其他很多资源，比如 sandbox 容器、volume 卷等等，在删除后都需要进行回收，而删除 Pod 最终也是去删除对应的容器，这个就需要 Pod 所在节点的 kubelet 来完成清理了。kubelet 首先同样会一直 watch 我们的 Pod，当 Pod 的删除时间更新后，自然就会接收到事件，然后进行相应的清理工作。\n kubelet 对 Pod 的处理主要在 syncLoop 函数中，会去调用和事件相关的处理函数 syncLoopIteration\n 当执行删除操作的时候，apiserver 首先会更新 Pod 中的 DeletionTimestamp 属性，这个改变对于 kubelet 来说属于更新操作，所以会对应 kubetypes.UPDATE 操作，会调用 HandlePodUpdates 函数进行更新。\n 在 HandlePodUpdates 中会调用 dispatchWork 将 Pod 删除分配给具体的 worker 处理，podWorker 是具体的执行者，也就是每次 Pod 需要更新都会发送给 podWorker。\n podWorker在接收到删除操作的UpdateOption之后，会进行killPod的方法调用。\n1 2 3 4 5 6 7 8 9 10  func (kl *Kubelet) killPod(pod *v1.Pod, p kubecontainer.Pod, gracePeriodOverride *int64) error {  // 调用容器运行时去停掉Pod中的容器  if err := kl.containerRuntime.KillPod(pod, p, gracePeriodOverride); err != nil {  return err  }  if err := kl.containerManager.UpdateQOSCgroups(); err != nil {  klog.V(2).InfoS(\u0026#34;Failed to update QoS cgroups while killing pod\u0026#34;, \u0026#34;err\u0026#34;, err)  }  return nil }    在kl.containerRuntime.KillPod 中，先杀掉所用运行的容器，然后删除Pod的SandBox，kill容器时，会为每个容器开启一个goroutine来进行删除，删除中会先执行pre-stop的hook，最后再停止，这一步才调用CRI来停止容器（pkg/kubelet/kuberuntime/kuberuntime_container.go line:717）。\n 容器停掉之后，接下来就会去调用运行时服务的 StopPodSandbox 函数停止 sandbox 容器。\n1 2 3 4 5 6 7  // Stop all sandboxes belongs to same pod for _, podSandbox := range runningPod.Sandboxes {  if err := m.runtimeService.StopPodSandbox(podSandbox.ID.ID); err != nil \u0026amp;\u0026amp; !crierror.IsNotFound(err) {  killSandboxResult.Fail(kubecontainer.ErrKillPodSandbox, err.Error())  klog.ErrorS(nil, \u0026#34;Failed to stop sandbox\u0026#34;, \u0026#34;podSandboxID\u0026#34;, podSandbox.ID)  } }    到这里 kubelet 就完成了对 Pod 的优雅删除，但是这并没有结束。\n同步删除结果  对于优雅删除一开始在 apiserver 只是给 Pod 设置了 DeletionTimestamp 属性，然后 kubelet watch 来更新后去完成了 Pod 的优雅删除，但是现在服务端中还有 Pod 的记录，并没有真正去删除。\n 在 kubelet 启动的时候同时还去启动了一个 statusManager 的同步循环，该 Manager 是 kubelet pod 状态的真实来源，应该与最新的 v1.PodStatus 保持同步，它还将更新同步回 apiserver，也就是当优雅删除完成后我们还将通过该管理器将状态同步回 apiserver。\n 状态管理器在与 apiserver 进行状态同步的时候会去调用该管理器下面的 syncPod 方法进行处理，代码位于 pkg/kubelet/status/status_manager.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  func (m *manager) Start() {  // Don\u0026#39;t start the status manager if we don\u0026#39;t have a client. This will happen  // on the master, where the kubelet is responsible for bootstrapping the pods  // of the master components.  if m.kubeClient == nil {  klog.InfoS(\u0026#34;Kubernetes client is nil, not starting status manager\u0026#34;)  return  }   klog.InfoS(\u0026#34;Starting to sync pod status with apiserver\u0026#34;)   //nolint:staticcheck // SA1015 Ticker can leak since this is only called once and doesn\u0026#39;t handle termination.  syncTicker := time.NewTicker(syncPeriod).C   // syncPod and syncBatch share the same go routine to avoid sync races.  go wait.Forever(func() {  for {  select {  case syncRequest := \u0026lt;-m.podStatusChannel:  klog.V(5).InfoS(\u0026#34;Status Manager: syncing pod with status from podStatusChannel\u0026#34;,  \u0026#34;podUID\u0026#34;, syncRequest.podUID,  \u0026#34;statusVersion\u0026#34;, syncRequest.status.version,  \u0026#34;status\u0026#34;, syncRequest.status.status)  ===============  m.syncPod(syncRequest.podUID, syncRequest.status)  ===============  case \u0026lt;-syncTicker:  klog.V(5).InfoS(\u0026#34;Status Manager: syncing batch\u0026#34;)  // remove any entries in the status channel since the batch will handle them  for i := len(m.podStatusChannel); i \u0026gt; 0; i-- {  \u0026lt;-m.podStatusChannel  }  m.syncBatch()  }  }  }, 0) }   在syncPod中会依次检查\n  是否有容器运行\n  volumes是否清理\n  pod cgroup 是否清理\n   检查成功将向 apiserver 发送 Delete 请求，再次删除 Pod 了。\n 不过这一次的设置的 GracePeriodSeconds 为 0，表示要强制删除 Pod 了，到这里 apiserver 会再次收到 DELETE 请求，与第一次不同的是，这次是强制删除 Pod，会去 etcd 中删除 Pod 对象了。\n 同时还会调用 probeManager 去移除 Pod 相关的探针 prober worker，到这一步接受就表示 Pod 彻底从节点上删除了。\n","permalink":"/posts/tech/pod%E5%88%9B%E5%BB%BA%E6%B5%81%E7%A8%8B/","summary":"这篇文章记录Pod的删除流程。kubernetes版志1.23.4 记录删除时间DeletionTimestamp 执行删除时，kubelet会","title":"Pod创建流程"}]